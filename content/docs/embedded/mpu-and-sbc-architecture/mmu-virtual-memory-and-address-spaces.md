---
title: "MMU, Virtual Memory & Address Spaces"
weight: 30
---

# MMU, Virtual Memory & Address Spaces

The MMU is the biggest conceptual leap from MCU to MPU. On an MCU, a pointer is a physical address. On an MPU running Linux, a pointer is a virtual address that the MMU translates to a physical address -- and that translation is what makes process isolation, memory protection, and modern operating systems possible.

## Why Virtual Memory Exists

On a microcontroller, every piece of code shares one flat physical address space. Your main loop, your ISRs, your RTOS tasks -- they all see the same memory, can write to the same addresses, and can corrupt each other freely. The Cortex-M MPU (Memory Protection Unit) can restrict which regions each privilege level can access, but it does not change what addresses mean. Address 0x20000000 is the same physical SRAM byte no matter who is reading it. See [Memory Map]({{< relref "/docs/embedded/mcu-architecture/memory-map" >}}) for how that flat physical address space works on Cortex-M.

Virtual memory exists to solve a set of problems that only emerge when you try to run multiple independent programs on the same hardware. The first problem is isolation: if two processes are running simultaneously, and one has a bug that writes to a wild pointer, it must not be able to corrupt the other process's data. The second is protection: user-space code must not be able to read or write kernel memory, because the kernel holds the security and scheduling state for the entire system. The third is convenience: each process should be able to pretend it has the entire address space to itself, without worrying about where other processes placed their data.

There are further practical benefits that follow from the same mechanism. Memory overcommit lets processes allocate (via malloc) more memory than physically exists -- the kernel only assigns physical pages when they are actually written to, not when they are requested. Memory-mapped files let you access file contents as if they were in memory, with the kernel handling the actual disk I/O behind the scenes. Shared libraries can be loaded once in physical memory and mapped into the virtual address space of every process that needs them, saving significant RAM on systems running dozens of processes.

All of this requires hardware support. The OS cannot intercept every memory access in software -- that would be impossibly slow. The MMU sits between the CPU and the memory bus, translating every virtual address to a physical address on the fly. Without it, none of these features are practical.

## The MMU: Page Tables and Translation

The MMU divides both virtual and physical memory into fixed-size chunks called pages. The standard page size on most architectures is 4 KB. When the CPU issues a memory access to virtual address 0x00007F4A, the MMU looks up which physical page that virtual page maps to, then accesses the corresponding physical address. The offset within the page (the low 12 bits, for a 4 KB page) passes through unchanged.

The mapping from virtual pages to physical pages is stored in a data structure called the page table. On 64-bit ARM (AArch64), this is a 4-level hierarchical table. Each level is itself a page-sized array of entries, and each entry either points to the next level or (at the final level) contains the physical page address plus permission bits (read, write, execute, user/kernel). The 4-level structure avoids the need to allocate table entries for the vast empty regions of the 48-bit or larger virtual address space -- only the populated portions of the tree need to exist in memory.

Larger page sizes are available for special purposes. Huge pages (2 MB on ARM and x86) and gigantic pages (1 GB) map large contiguous regions with fewer table entries and fewer TLB slots consumed. Embedded Linux systems with large contiguous buffers -- frame buffers, DMA regions, machine learning model weights -- benefit from huge pages because they reduce TLB pressure. However, huge pages complicate memory management and can lead to allocation failures if physical memory is fragmented.

The critical performance concern with page table translation is that every memory access conceptually requires multiple memory accesses just to walk the table. A 4-level page table means four additional reads before the actual data can be fetched. This would be catastrophically slow if it happened on every access. The solution is the TLB -- the Translation Lookaside Buffer -- a small, fast cache that stores recent virtual-to-physical mappings. When the CPU accesses an address whose mapping is already in the TLB, translation happens in a single cycle with no table walk. TLB hit rates in practice are typically 99% or higher, because programs exhibit strong spatial and temporal locality.

When the TLB does not contain the mapping -- a TLB miss -- the hardware (on ARM) or the OS (on some other architectures) must walk the page table to find the mapping and load it into the TLB. This is called a page table walk, and it costs dozens to hundreds of cycles depending on whether the page table entries are in cache. On ARM, the page table walk is performed by hardware in the MMU, which means the OS does not need to handle every TLB miss, but the latency is still significant.

One additional mechanism worth understanding is the ASID -- Address Space Identifier. The TLB can hold entries from multiple processes simultaneously, tagged by ASID, so that a context switch does not require flushing the entire TLB. Without ASID, every context switch would invalidate all TLB entries, and the new process would start with a cold TLB, paying the cost of table walks for every address until the TLB warmed up. ARM allocates 8 or 16 bits for the ASID, giving 256 or 65536 possible values. When ASIDs are exhausted (more processes than IDs), the kernel must flush the TLB and reassign IDs, which is a performance hit on systems running many processes.

## Address Spaces in Practice

Each process on a Linux system sees its own flat virtual address space. On 64-bit ARM, this is typically a 48-bit space (256 TB), though most processes use only a tiny fraction of it. The key insight is that two processes can have the same virtual address -- say 0x00400000 -- and it maps to completely different physical pages for each. The page tables are different, the physical backing is different, and neither process can see the other's data.

The virtual address space is divided into regions by convention. The lower portion belongs to user space: the text segment (executable code), the data segment (initialized global variables), BSS (zero-initialized globals), the heap (growing upward from the end of data), memory-mapped regions (libraries, files), and the stack (growing downward from high addresses). The upper portion is reserved for the kernel. On 64-bit ARM Linux, the split is at 0x0000FFFF_FFFFFFFF -- user space gets the lower 256 TB, kernel space gets the upper 256 TB. On 32-bit ARM, the default split is 3 GB user / 1 GB kernel, which is surprisingly limiting for both sides in some workloads.

When a process calls fork(), the kernel creates a new process with a copy of the parent's address space. But it does not actually copy all the physical pages -- that would be enormously expensive. Instead, it uses copy-on-write: both parent and child initially share the same physical pages, marked read-only. When either process tries to write to a shared page, the MMU triggers a page fault, and the kernel allocates a new physical page, copies the data, and updates the writing process's page table to point to the new page. This makes fork() fast (only the page tables are copied, not the data) and memory-efficient (only pages that are actually modified get duplicated).

Inspecting a process's memory layout directly is a useful exercise. On any Linux system, `/proc/self/maps` (or `/proc/<pid>/maps`) shows the virtual memory regions for a process: the start and end address, permissions (read/write/execute), the mapped file (if any), and the offset. Reading this file on a Raspberry Pi demystifies what virtual memory actually looks like in practice -- the output shows the executable, the shared libraries (libc, libpthread), the heap, the stack, and the kernel's VDSO (virtual dynamic shared object) page. It is one of the most instructive things to examine when learning about virtual memory.

## Memory-Mapped I/O in an MMU World

On a microcontroller, writing to a peripheral register is just a store to a physical address. The GPIO data register might live at 0x40020014, and you write directly to it. There is nothing between your code and the hardware. This directness is what makes MCU programming feel so tangible.

On an MPU running Linux, user-space code cannot access physical addresses directly. The process's virtual address space has no mapping for physical peripheral addresses -- any attempt to access them results in a segmentation fault (the MMU detects there is no valid page table entry and raises a fault). This is a feature, not a limitation. If any user-space process could write to arbitrary physical addresses, one buggy or malicious program could crash the entire system by clobbering hardware registers.

The quick-and-dirty way to access hardware from user space is through `/dev/mem`. This special device file represents physical memory. A root-privileged process can open it, then use mmap() to map a range of physical addresses into its virtual address space. After the mmap call, the process has a pointer it can use to read and write hardware registers directly. This approach works for quick GPIO toggling experiments on a Raspberry Pi -- but it is dangerous. There are no protections: you can map and corrupt kernel memory, DMA buffers, or any other physical address. The kernel has no idea you are touching the hardware, so its driver state and your direct access will conflict.

The proper way to access hardware is through kernel drivers. A driver runs in kernel space, where it can map physical peripheral addresses into kernel virtual space using `ioremap()`. The driver then exposes a controlled interface to user space -- a character device in `/dev`, attributes in `/sys`, or a platform device that other kernel subsystems can use. User-space code talks to the driver through standard system calls (open, read, write, ioctl), and the driver mediates all hardware access. This is the model that keeps the system stable. See [Drivers, Kernel Space & User Space]({{< relref "drivers-kernel-space-and-user-space" >}}) for how the driver model works.

Between `/dev/mem` and a full kernel driver, there are middle-ground approaches. UIO (Userspace I/O) is a kernel framework that maps a single device's registers into user space with minimal kernel code. GPIO access through `libgpiod` uses the kernel's GPIO subsystem but provides a clean user-space API. SPI and I2C devices can be accessed from user space through `/dev/spidevX.Y` and `/dev/i2c-X`. These are all built on the same principle: the kernel sets up the MMU mappings and access controls, and user space gets a safe, mediated path to the hardware.

## Swap, OOM, and the Embedded Tradeoff

Virtual memory creates an abstraction where the system can promise more memory than physically exists. When a process allocates memory, the kernel may not immediately back it with physical pages -- it just reserves virtual address space. Physical pages are only assigned when the process actually writes to them (this is demand paging). This means the sum of all process allocations can exceed physical RAM, a behavior called overcommit.

On a desktop system, swap extends this further. When physical RAM fills up, the kernel can write infrequently-used pages to a swap partition or file on disk, freeing physical RAM for active pages. If those pages are accessed again later, the kernel reads them back from swap, possibly evicting other pages. This creates the illusion of more memory than the hardware provides, at the cost of occasional disk I/O latency.

On embedded Linux systems, swap is almost always a bad idea. The storage is typically an SD card or eMMC, both of which have limited write endurance. Active swapping can burn through an SD card's write cycles in weeks or months. The latency is also brutal -- an SD card read takes milliseconds, compared to nanoseconds for DRAM. A system that is actively swapping becomes unusably slow and unpredictable. There are documented cases of embedded systems with swap enabled on an SD card where the card wore out within six months of deployment. The fix is simple: do not enable swap on embedded systems with flash-based storage.

When physical memory is truly exhausted and there is no swap (or swap is full), the kernel invokes the OOM (Out of Memory) killer. The OOM killer selects a process to terminate and kills it to free memory. The selection heuristic considers the process's memory usage, its oom_score_adj setting, and various other factors, but the result is often surprising -- it may kill the process you care about most, or it may kill a system service that other processes depend on. On an embedded system with a fixed, well-defined workload, the OOM killer should never fire. If it does, it means the memory budget was wrong.

Linux provides the `vm.overcommit_memory` sysctl to control overcommit behavior. The default (mode 0) allows moderate overcommit based on heuristics. Mode 1 allows unlimited overcommit (malloc never fails). Mode 2 limits total virtual memory to swap plus a configurable percentage of physical RAM -- this is the safest setting for embedded systems because it makes malloc fail early rather than allowing processes to allocate memory that cannot be backed, only to be killed later by the OOM killer. Many embedded Linux configurations set mode 2 with no swap, which means processes get honest answers from malloc about whether memory is available.

## Gotchas

- **TLB flush on context switch is expensive** -- When the kernel switches between processes, TLB entries from the old process are invalid for the new one. ASID tagging avoids a full flush in many cases, but ASID space is limited and a full flush is still needed when ASIDs are recycled. On real-time-sensitive embedded systems, TLB flush latency contributes to context switch jitter
- **/dev/mem access bypasses all kernel protections and can crash the system** -- Mapping physical memory into user space lets you read or write anything: kernel data structures, DMA buffers, peripheral registers the kernel thinks it owns exclusively. Conflicting with a kernel driver this way causes data corruption, lockups, or panics. Use it for quick experiments, never in production
- **Page faults cause non-deterministic latency** -- A page fault (accessing a valid virtual page that is not currently backed by a physical page) triggers kernel code to allocate a page, possibly zeroing it, possibly reading it from disk. This takes microseconds to milliseconds. For hard real-time tasks, all memory should be pre-faulted (mlockall) to eliminate this source of jitter
- **Swap on SD cards kills the card and the performance** -- Flash storage has limited write cycles, and swap generates enormous write traffic. Even when the card survives, swap latency (milliseconds) compared to DRAM latency (nanoseconds) makes the system feel like it has frozen. Disable swap on embedded systems and budget physical RAM instead
- **OOM killer picks victims heuristically and may kill the wrong process** -- The heuristic is reasonable for general-purpose systems but often wrong for embedded: it may kill your main application while leaving a logging daemon alive. Tune oom_score_adj for critical processes (set to -1000 to make them immune) and fix the memory budget so OOM never fires
- **Kernel and user space share the same virtual address range** -- The kernel is mapped into the upper portion of every process's virtual address space so that system calls do not require a full page table switch. This sharing is efficient but means speculative execution attacks (Meltdown) could read kernel memory from user space. Kernel Page Table Isolation (KPTI) mitigates this at a performance cost. On embedded ARM processors, check whether your SoC is affected
- **32-bit ARM has a 3G/1G user/kernel split by default** -- User space gets 3 GB, the kernel gets 1 GB. Processes needing more than 3 GB of virtual address space are out of luck, and the kernel is constrained to 1 GB for all its mappings (including `ioremap` for device memory). This limit is one of many reasons 64-bit ARM (AArch64) is strongly preferred for new embedded Linux designs
- **Faults manifest differently on MCUs and MPUs** -- On a Cortex-M, a bad memory access triggers a HardFault or BusFault that you handle in firmware. On an MPU running Linux, the MMU raises a page fault that the kernel handles -- delivering a SIGSEGV to the offending process, logging it, and killing just that process while the rest of the system continues. Same underlying problem, very different failure mode and recovery path. See [Faults & Exceptions]({{< relref "/docs/embedded/embedded-reality/faults-and-exceptions" >}}) for how faults work on the MCU side
