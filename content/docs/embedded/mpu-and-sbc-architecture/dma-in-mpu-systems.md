---
title: "DMA in MPU Systems"
weight: 80
---

# DMA in MPU Systems

DMA on an MCU means configuring a peripheral to read from or write to physical memory addresses — you set up source, destination, and length in registers, and the transfer runs without CPU intervention. DMA on an MPU running Linux involves the same concept but adds layers of complexity: virtual-to-physical address translation, IOMMU protection, kernel-managed DMA mappings, and cache coherency protocols. The hardware problem is the same — move data without burning CPU cycles — but the software machinery is fundamentally different.

## DMA with an MMU

The core problem is straightforward once you state it: DMA controllers operate on physical addresses, but the kernel and user space work with virtual addresses. A buffer that a user-space program allocated with `malloc()` has a virtual address. That virtual address means nothing to the DMA controller — it is an index into that process's page table, not a location on the memory bus. If you somehow passed that virtual address to a DMA controller, it would read or write whatever happened to live at that numerical value in physical memory, which is almost certainly not the buffer you intended. On an MCU, this entire problem does not exist because all addresses are physical. See [DMA Fundamentals]({{< relref "/docs/embedded/communication-interfaces/dma" >}}) for how DMA works in that simpler world.

The kernel must therefore translate virtual addresses to physical addresses before programming a DMA transfer. But translation alone is not enough. The kernel's virtual memory system is free to move physical pages, swap them out to disk, or reclaim them if they are not in use. If the kernel swaps out a page while a DMA transfer is reading from it, the DMA controller sees whatever data the kernel put there next — or an unmapped region of physical memory. The solution is page pinning: the kernel locks the physical pages backing a DMA buffer so they cannot be moved, reclaimed, or swapped for the duration of the transfer. On an MCU, memory does not move, so there is nothing to pin.

There is a further subtlety that is easy to miss at first. Virtual memory is allocated in pages (typically 4 KB), and a buffer that is contiguous in virtual memory is not necessarily contiguous in physical memory. A 16 KB buffer might span four virtual pages that map to four non-adjacent physical pages scattered across DRAM. The DMA controller, working in the physical domain, sees four separate chunks, not one contiguous buffer. This is the scatter-gather problem, and it matters for any DMA transfer larger than a single page. See [MMU, Virtual Memory & Address Spaces]({{< relref "mmu-virtual-memory-and-address-spaces" >}}) for the details of virtual-to-physical translation and page tables.

## IOMMU and DMA Mapping

Some SoCs include an IOMMU — an I/O Memory Management Unit — which is essentially an MMU for DMA controllers. It sits between the device and the memory bus and translates bus addresses (the addresses the device uses) to physical addresses, just as the CPU's MMU translates virtual addresses to physical addresses. The IOMMU also enforces access permissions: a device can only DMA to the physical pages that the IOMMU has been configured to allow. Without an IOMMU, a buggy driver or a misbehaving device can DMA to any physical address, including kernel text, page tables, or another process's memory. That is not a theoretical concern — it is a well-known attack surface, and it is why IOMMU support matters for security even on embedded systems.

The Linux kernel provides a DMA API that abstracts the details of address translation, IOMMU configuration, and platform-specific quirks. The two main allocation paths are coherent mappings and streaming mappings. `dma_alloc_coherent()` allocates a buffer and returns two values: a kernel virtual address for the CPU to use and a DMA address (bus address) for the device. The memory is set up so that both CPU and device see a consistent view of the data at all times, which simplifies programming but has performance implications discussed in the next section. This is the right choice for buffers that both the CPU and device access frequently and unpredictably — command rings, descriptor tables, shared status registers.

`dma_map_single()` and `dma_map_sg()` take a different approach. Instead of allocating new memory, they map an existing buffer for DMA access. `dma_map_single()` handles a single contiguous buffer. `dma_map_sg()` handles scatter-gather lists — arrays of buffer fragments that may be physically non-contiguous. These are streaming mappings: they have a direction (to-device, from-device, or bidirectional) and require explicit synchronization between CPU and device access. Streaming mappings are used for data that flows in one direction at a time — network packet buffers, disk I/O blocks, audio sample buffers — and they allow the kernel to use cached memory, which is significantly faster for CPU access.

One more piece of the puzzle is the Contiguous Memory Allocator (CMA). Some devices cannot do scatter-gather DMA — they need a single physically contiguous buffer. Video codec engines that need a 4 MB frame buffer, display controllers that DMA an entire framebuffer line by line, and camera interfaces that capture directly to memory all fall into this category. CMA reserves a pool of physical memory at boot time that can be used by normal allocations when DMA is not active, but can be reclaimed (by migrating those normal allocations elsewhere) when a large contiguous DMA buffer is needed. The pool size is configured at boot via kernel parameters or device tree, and it cannot be expanded later — if the pool is too small, large contiguous allocations fail at runtime with no fallback.

## Cache Coherency and DMA

This is the same fundamental problem that exists on MCUs with data caches, but with more layers and more ways to get it wrong. The core issue is that the CPU and the DMA controller access DRAM through different paths. The CPU accesses memory through its cache hierarchy — when it reads an address, the data may come from L1 or L2 cache rather than DRAM. When it writes, the data may sit in cache for a while before being written back to DRAM. The DMA controller bypasses the CPU's caches entirely and reads from or writes to DRAM directly. This creates two stale-data scenarios. See [DMA Fundamentals]({{< relref "/docs/embedded/communication-interfaces/dma" >}}) for how this same problem manifests on Cortex-M7 MCUs with caches.

In the first scenario, the CPU writes data to a buffer, and a DMA controller needs to read it (a DMA-to-device transfer, like transmitting a network packet). If the CPU's writes are still sitting in cache and have not been flushed to DRAM, the DMA controller reads stale data from DRAM. The CPU thinks the buffer contains the packet it just assembled, but the DMA controller sees whatever was in DRAM before the writes — possibly zeros, possibly data from a previous transfer. The fix is to flush (clean) the cache lines covering the buffer before starting the DMA transfer, forcing the CPU's cached writes out to DRAM.

In the second scenario, a DMA controller writes data to a buffer (a DMA-from-device transfer, like receiving an ADC sample block), and the CPU then needs to read it. The DMA writes go directly to DRAM, but the CPU's cache may still hold stale copies of the old data at those addresses. When the CPU reads the buffer, it gets the cached old data instead of the fresh DMA data. The fix is to invalidate the cache lines covering the buffer after the DMA transfer completes, forcing the CPU to re-fetch from DRAM on the next access.

The Linux DMA API handles these cache operations, but only if you use it correctly. Coherent mappings (`dma_alloc_coherent()`) sidestep the problem by marking the memory as uncacheable — the CPU always reads and writes directly to DRAM, so there is nothing to get out of sync. This is simple and correct, but uncacheable memory is slow for CPU access. If the CPU needs to iterate over a large buffer (processing received samples, assembling a packet), the performance hit from bypassing the cache can be severe. Streaming mappings use cached memory but require explicit sync calls: `dma_sync_single_for_device()` before a to-device transfer (to flush CPU caches to DRAM) and `dma_sync_single_for_cpu()` after a from-device transfer (to invalidate CPU caches). Forgetting a sync call produces intermittent data corruption — intermittent because it depends on whether the relevant cache lines happen to be dirty or present, which varies with system load, timing, and memory pressure.

Some SoCs provide hardware cache coherency for DMA through interconnect protocols like ARM's ACE or CHI. On these systems, the DMA controller's memory accesses participate in the cache coherency protocol — the hardware snoops the CPU's caches automatically, and no software flush or invalidate is needed. This is the ideal situation, but it is not universal. Many mid-range embedded SoCs (including some Allwinner, Rockchip, and Broadcom parts) have non-coherent DMA ports for at least some peripherals, so the software cache management path remains essential to understand.

## DMA in User Space

For the vast majority of embedded Linux work — talking to SPI devices, reading I2C sensors, driving UARTs — DMA is invisible to user-space code. The kernel driver configures and manages DMA transfers internally, and user space interacts with the driver through standard file operations (read, write, ioctl) or through frameworks like spidev and i2c-dev. You never allocate a DMA buffer, never program a DMA address, never think about cache coherency. The driver handles all of it. See [Drivers, Kernel Space & User Space]({{< relref "drivers-kernel-space-and-user-space" >}}) for how this abstraction works.

User-space DMA exists for specialized high-performance scenarios where the overhead of copying data between kernel and user space is unacceptable. DPDK (Data Plane Development Kit) is the most prominent example in networking: it bypasses the kernel's network stack entirely, mapping NIC DMA buffers directly into a user-space process. Packet processing happens entirely in user space, avoiding system call overhead, context switches, and data copies. This can achieve line-rate packet processing that the kernel's standard networking path cannot match, but it requires dedicating CPU cores to polling and surrendering the kernel's networking features (firewall, routing, socket API).

VFIO (Virtual Function I/O) provides a more general mechanism for assigning a device to user space or to a virtual machine. VFIO uses the IOMMU to create a protected DMA context: the user-space process can program DMA transfers, but the IOMMU restricts the device to accessing only the physical pages that have been explicitly mapped. This provides the performance of direct hardware access with the safety of IOMMU isolation. VFIO is the foundation for device passthrough in virtualization (assigning a GPU or NIC to a VM), but it also serves user-space driver frameworks in embedded systems.

Video4Linux2 (V4L2) represents a middle ground that is common in embedded camera and video applications. V4L2 drivers allocate DMA buffers in kernel space, and user space accesses them through `mmap()`. The kernel maps the DMA buffer pages into the user-space process's address space, so the CPU can read the captured frames without a copy. The DMA setup, cache management, and buffer rotation are all handled by the kernel driver — user space just calls `ioctl()` to queue and dequeue buffers and reads the mapped memory to access pixel data. This pattern of kernel-managed DMA with user-space mmap access shows up in audio (ALSA), networking (packet mmap), and GPU (DRM/KMS) subsystems as well. For typical embedded work, understanding that these frameworks exist and handle the DMA complexity is more important than knowing the implementation details.

## Gotchas

- **Using virtual addresses for DMA is a guaranteed bug** — If you pass a virtual address to a DMA controller (or to a register that expects a bus/physical address), the DMA controller will read or write the wrong physical location. The data corruption may be silent, or the access may hit an unmapped region and cause a bus error. This mistake is easy to make when porting MCU code (where all addresses are physical) to an MPU environment
- **Forgetting to sync caches with streaming DMA mappings causes intermittent data corruption** — This is the hardest kind of bug to diagnose because it depends on cache state, which varies with system load, timing, and memory layout. The corruption appears and disappears seemingly at random, passes in testing and fails in the field. Always pair `dma_map` with the corresponding `dma_sync` calls
- **Coherent DMA memory is uncacheable and slow for CPU access** — `dma_alloc_coherent()` memory bypasses the CPU cache, so every CPU read and write goes directly to DRAM. Do not use it as general-purpose working memory. If the CPU needs to process the data extensively, use streaming mappings with explicit sync instead
- **CMA pool size is fixed at boot** — The Contiguous Memory Allocator reserves its pool during early boot based on kernel parameters or device tree configuration. If the pool is undersized, large contiguous allocations fail at runtime and there is no way to expand it without rebooting. Size the pool for worst-case concurrent allocation, not average use
- **DMA from user space without IOMMU protection is a security hole** — A user-space process with direct DMA access can read or write any physical address, including kernel memory. Without IOMMU enforcement, a compromised process could escalate privileges or exfiltrate data by programming a DMA controller to copy kernel memory to a user-accessible buffer
- **Bounce buffers silently halve DMA throughput** — When a device cannot address all physical memory (common with 32-bit DMA masks on systems with more than 4 GB of RAM), the kernel allocates a temporary buffer in addressable memory, copies data to or from it, and programs the DMA to use the temporary buffer. This double-copy is transparent to the driver but cuts effective throughput. Check `dmesg` for bounce buffer warnings and consider using `dma_set_mask()` to declare the device's actual addressing capability
- **32-bit ARM DMA zones add another layer of constraints** — On 32-bit ARM, some devices can only DMA to the first 256 MB or first 4 GB of physical memory, depending on their bus connections. The kernel uses DMA zones to allocate from the right physical address range, but zone exhaustion (too many allocations in the restricted zone) can cause DMA allocation failures even when plenty of total memory is free. This is one more reason to prefer 64-bit ARM for new designs
