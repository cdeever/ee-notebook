---
title: "CPU Cores & SoC Architecture"
weight: 20
---

# CPU Cores & SoC Architecture

Application processor cores and the SoCs built around them are a different world from MCU cores -- deeper pipelines, caches, out-of-order execution, and silicon blocks that have nothing to do with the CPU itself. Moving from a Cortex-M to a Cortex-A part is not just a step up in clock speed. The core microarchitecture changes fundamentally, and the chip surrounding the core grows into an entire computing platform on a single die. Understanding what that platform contains -- and what each block does -- is essential for making informed decisions about which SoC fits a given project.

## Cortex-A: The Application Processor Core

If you are coming from Cortex-M territory, the Cortex-A family feels like a different species. The Cortex-M cores described in [Core Architectures]({{< relref "/docs/embedded/mcu-architecture/core-architectures" >}}) are designed around determinism: short pipelines (2-6 stages), in-order execution, single-issue, and predictable cycle counts. The Cortex-A cores prioritize throughput. They execute instructions out of order, speculatively fetch down both sides of a branch, issue multiple instructions per cycle, and rely on deep pipelines to sustain high clock frequencies.

A Cortex-A53 has an 8-stage pipeline. A Cortex-A76 has around 13 stages. The A710 and beyond push deeper still. These are not just "more stages" of the same thing -- the pipeline stages include rename, dispatch, and retirement logic that does not exist in a Cortex-M. The hardware maintains a reorder buffer to track instructions that are executing out of order and ensure that their results commit in the correct program order. This is a significant amount of silicon dedicated to making the core fast while maintaining the illusion of sequential execution.

Out-of-order execution means the core examines a window of upcoming instructions and dispatches them to execution units as operands become available, regardless of program order. This hides memory latency -- while one instruction waits for a cache miss, others continue executing. Superscalar issue means the core can dispatch two or more instructions in the same cycle to different execution units (integer ALU, floating-point, load/store, branch). The result is dramatically higher instructions-per-clock (IPC) than a Cortex-M, but at the cost of predictability. You cannot count cycles on a Cortex-A the way you can on an M0.

Branch prediction is another major difference. Cortex-M3 and above have simple branch predictors, but Cortex-A cores use sophisticated multi-level predictors with large branch target buffers. A mispredicted branch on a deep pipeline costs many cycles to flush and refill -- 15+ cycles on some Cortex-A designs versus 1-3 on Cortex-M. The predictor's job is to make mispredictions rare, and modern Cortex-A cores achieve prediction accuracy above 95%. But the penalty when it misses is severe, and this is one reason that hard real-time code does not belong on these cores.

The specific cores worth knowing fall into two camps. The efficiency cores -- A7, A53, A55 -- are in-order (A7, A53) or modest out-of-order (A55) designs that trade peak performance for low power and small die area. The A53 is probably the single most widely deployed 64-bit core in the world; it appears in everything from cheap Android phones to networking equipment to embedded Linux SBCs. The performance cores -- A72, A76, A77, X1 and beyond -- are wide, aggressive out-of-order designs that chase single-threaded performance. The A72 was the step where ARM became competitive with mid-range x86 cores for many workloads. The A76 and its successors widened the gap further. For the SBC world, the A72 (Raspberry Pi 4, Pi 5's BCM2712 uses A76) and A55 (budget SBCs) are the cores you encounter most often.

A notable point when transitioning from the MCU world: the instruction set itself changes at the Cortex-A boundary. Cortex-M cores run Thumb-2 (a mix of 16-bit and 32-bit encodings) and cannot execute classic 32-bit ARM instructions. Cortex-A cores historically supported both ARM and Thumb modes, though the newer AArch64 (ARMv8-A 64-bit) execution state uses a completely new fixed-width 32-bit instruction encoding called A64. When you see a Cortex-A53 or A72, it is running AArch64 with the A64 instruction set under a 64-bit Linux kernel, though it can still run 32-bit AArch32 code in compatibility mode. The toolchain and ABI are different from Cortex-M -- cross-compiling for `aarch64-linux-gnu` instead of `arm-none-eabi`. This is not just a flag change; it means different calling conventions, different register names (X0-X30 instead of R0-R15), and a different exception model.

Understanding the [MCU vs MPU decision]({{< relref "mcu-vs-mpu" >}}) helps frame why these differences matter. You choose a Cortex-A because you need the throughput, the memory management hardware, and the software ecosystem -- not because you need deterministic timing.

## Multi-Core and big.LITTLE

Most application processors ship with multiple CPU cores, and the marketing claims can be misleading. A "quad-core A72" does not deliver four times the performance of a single A72. Amdahl's law is brutal: only the parallelizable portion of your workload benefits from additional cores. A single-threaded application runs on one core while the other three sit idle. Even well-threaded applications hit synchronization overhead, cache contention, and memory bandwidth limits. In practice, going from one core to four might yield 2-3x throughput for well-parallelized workloads, and nearly zero benefit for single-threaded code.

Where multiple cores genuinely help is concurrent workloads -- not one task split into threads, but multiple independent tasks running simultaneously. An embedded Linux system might run a web server on one core, a sensor acquisition daemon on another, a database on a third, and the display compositor on a fourth. Each is single-threaded, but the system as a whole benefits from parallelism. This is the common case on SBC-based embedded designs, and it is where quad-core SoCs earn their keep. The kernel mediates the sharing, and tools like `htop` or `mpstat` reveal how load distributes across cores in practice.

Symmetric multiprocessing (SMP) is the standard model on homogeneous multi-core SoCs. All cores are identical, share the same view of memory, and can run any thread. The Linux scheduler assigns threads to cores and migrates them as load shifts. This is transparent to most user-space code -- you write threaded programs and the kernel handles placement.

But "transparent" does not mean "free." Cache coherency traffic between cores consumes bus bandwidth, and a thread migrating from one core to another pays a penalty as its working set must be fetched into the new core's L1 cache. On a busy quad-core system, coherency snoop traffic can consume a meaningful fraction of the memory bus bandwidth.

ARM's big.LITTLE architecture (and its successor DynamIQ) takes multi-core further by combining heterogeneous cores on a single SoC. A typical configuration pairs high-performance cores (e.g., A76) with efficiency cores (e.g., A55). The idea is that background tasks -- system services, idle polling, light I/O -- run on the small, power-efficient cores, while compute-heavy tasks get dispatched to the big cores. The scheduler (Linux's Energy Aware Scheduling, or EAS) makes the placement decisions based on load history and power targets. DynamIQ improved on the original big.LITTLE by allowing big and LITTLE cores to share an L3 cache within the same cluster, reducing the latency penalty when migrating threads between core types. Earlier big.LITTLE implementations used separate clusters with independent L2 caches, making migration more expensive.

Real examples illustrate the range. The Broadcom BCM2711 (Raspberry Pi 4) is a straightforward 4x Cortex-A72 SMP design -- all cores identical, no big.LITTLE complexity. The Allwinner H616 (Orange Pi Zero 2, many budget boards) is 4x Cortex-A53, also homogeneous, optimized for cost and low power. The Rockchip RK3588, found in higher-end SBCs and edge compute devices, uses a DynamIQ arrangement: 4x Cortex-A76 performance cores plus 4x Cortex-A55 efficiency cores. That eight-core configuration is powerful on paper but only delivers its full potential with software that is aware of the asymmetry. Running a single-threaded build process on an RK3588 will use one A76 core and ignore the other seven.

The power implications of big.LITTLE are significant for embedded and SBC applications. A Cortex-A55 core at 1.0 GHz might consume 100-200 mW. A Cortex-A76 core at 2.4 GHz might draw 2-4 W under load. On a passively cooled SBC, keeping most work on the efficiency cores dramatically extends thermal headroom. This is why the scheduler matters -- poor scheduling on a big.LITTLE chip wastes power and generates unnecessary heat.

big.LITTLE is known to cause confusion in benchmarking. Running a single-threaded benchmark on a big.LITTLE SoC may initially land on a LITTLE core (because the scheduler has not yet recognized the workload as heavy), producing underwhelming numbers for the first few hundred milliseconds before the scheduler migrates the thread to a big core. Conversely, pinning a thread to a specific core with `taskset` bypasses the scheduler entirely, which is useful for testing but does not reflect how production workloads behave. Understanding the scheduler's behavior is as important as understanding the hardware when evaluating big.LITTLE performance.

## The SoC: More Than a CPU

The term "System on Chip" is literal -- the CPU core, even a quad-core cluster, is often a minority of the die area on a modern SoC. The rest is a collection of specialized hardware blocks, each designed to handle a specific class of work more efficiently than the CPU could in software. A modern mid-range SoC might integrate a dozen or more distinct hardware engines alongside the CPU cluster. Understanding what lives on the SoC matters because these blocks determine what the system can do without external chips, and their presence (or absence) often drives SoC selection more than the CPU core itself.

The GPU is usually the largest non-CPU block. On Broadcom SoCs, it is the VideoCore architecture (VideoCore IV on BCM2835, VideoCore VI on BCM2711, VideoCore VII on BCM2712). On Allwinner and Rockchip parts, ARM's Mali GPU is common (Mali-400 on older parts, Mali-G52 or G610 on newer ones). The GPU handles display output, 2D/3D rendering, and increasingly, general-purpose compute (GPGPU) through APIs like OpenCL or Vulkan. For embedded Linux systems, the GPU also drives the display pipeline -- compositing windows, rendering UI frameworks, hardware-accelerated video playback.

Video encode and decode are handled by a dedicated Video Processing Unit (VPU), separate from the GPU. Hardware video decoding (H.264, H.265/HEVC, VP9) is critical for media applications because software decoding at 1080p or 4K would saturate the CPU. The VPU decodes compressed video frames with a fraction of the power the CPU would need. Some SoCs also include hardware video encoding for camera and streaming applications. On newer high-end SoCs, a Neural Processing Unit (NPU) or AI accelerator handles inference for machine learning models -- the RK3588, for instance, includes a 6-TOPS NPU for edge AI workloads.

Other common blocks include: a camera Image Signal Processor (ISP) for converting raw Bayer sensor data into usable images, crypto/security engines for hardware-accelerated AES/SHA and secure boot, a display controller for driving HDMI/DSI/LVDS outputs, and bus controllers for USB, PCIe, SDIO (SD card), MIPI-CSI (camera), and MIPI-DSI (display). Each of these blocks has its own registers, DMA channels, and interrupt lines, and the Linux kernel needs a driver for each one.

The practical consequence is that an SoC's value depends heavily on driver support. A chip may integrate a capable hardware video decoder, but if the Linux kernel does not have a working driver for it, you are stuck with CPU-based software decoding. There are SBCs with impressive hardware spec sheets -- 4K decode, hardware crypto, camera ISP -- where half the blocks are unusable under mainline Linux because the vendor only provided drivers for their Android BSP. This is why community Linux support (mainline kernel drivers, active mailing list, upstream device tree bindings) matters as much as the silicon itself when evaluating an SoC for an embedded Linux project.

Connecting all of these blocks is the interconnect fabric -- typically based on ARM's AMBA bus protocols. AXI (Advanced eXtensible Interface) handles high-bandwidth paths between the CPU, memory controller, GPU, and DMA-capable peripherals. AHB and APB handle lower-bandwidth peripheral access. The ACE (AXI Coherency Extensions) protocol maintains cache coherency between the CPU cores and DMA masters. The interconnect is invisible from user space, but it determines the bandwidth and latency characteristics of the entire system. When a DMA transfer from a camera ISP competes with the GPU for memory bandwidth, it is the interconnect arbitration that decides who waits.

One way to think about it: on an MCU, the CPU *is* the chip, with peripherals attached. On an SoC, the CPU is one client on a shared bus, competing with a GPU, a VPU, camera pipelines, and DMA engines for access to the same DRAM. The CPU might be the most programmable block, but it is rarely the most bandwidth-hungry. A 4K video decode pipeline or a GPU rendering a display frame can dominate the memory bus, leaving the CPU starved for bandwidth. This is why SoC vendors spend significant design effort on the interconnect and memory controller -- they are the shared resources that determine whether the whole system works smoothly or stutters under load.

## Memory Hierarchy: Caches and Beyond

The memory hierarchy on a Cortex-A SoC is fundamentally different from the flat, deterministic memory map on a Cortex-M. On an MCU, every memory access takes a known number of cycles -- one cycle for SRAM, a few cycles for flash with wait states, all documented in the datasheet. On a Cortex-A, the time to access memory depends on where the data currently lives in a multi-level cache hierarchy, and that changes dynamically as the program executes.

The cache hierarchy is where Cortex-A performance and Cortex-A unpredictability both originate. Every Cortex-A core has private L1 caches: an instruction cache (I-cache) and a data cache (D-cache), typically 32 KB or 64 KB each. These operate at core clock speed and service most memory accesses in 1-4 cycles. An L1 cache miss falls through to the L2 cache, which is either per-core (on some designs) or shared across a cluster of cores. L2 sizes range from 256 KB to 2 MB. Some SoCs add a shared L3 cache (often called a system-level cache or SLC) that sits between the L2 and the external DRAM controller.

Cache coherency is maintained automatically by hardware protocols -- MESI (Modified, Exclusive, Shared, Invalid) or its extension MOESI. When one core writes to a cache line that another core has cached, the coherency protocol ensures the stale copy is invalidated or updated. This happens transparently but not for free. Coherency traffic consumes bus bandwidth and adds latency, especially when multiple cores are actively sharing and modifying the same data. False sharing -- where two cores access different variables that happen to share a cache line (typically 64 bytes on Cortex-A) -- is a subtle performance killer in multi-core systems. Two threads updating adjacent fields in a struct can thrash a shared cache line back and forth between cores, turning what should be independent operations into a serialized bottleneck.

For embedded work, cache behavior matters in two specific ways. First, DMA coherency: when a peripheral DMA engine writes data to DRAM, the CPU's cache may still hold stale data for those addresses. The kernel must invalidate the relevant cache lines before the CPU reads DMA'd data, and flush (clean) cache lines before a DMA read from memory. Getting this wrong produces data corruption that appears intermittent and is extremely difficult to debug. See [MMU, Virtual Memory & Address Spaces]({{< relref "mmu-virtual-memory-and-address-spaces" >}}) for how caches interact with the virtual memory system.

Second, real-time jitter. A cache hit takes 1-4 cycles; a last-level cache miss that goes to DRAM takes 50-200+ cycles. If your code path sometimes hits in cache and sometimes misses, execution time varies by an order of magnitude. This is the fundamental reason Cortex-A cores are not suitable for hard real-time work without careful cache management. Locking critical code and data into cache (cache lockdown, supported on some Cortex-A cores) can help, but it reduces the effective cache size for everything else and is not straightforward to configure.

Some Cortex-A parts include Tightly Coupled Memory (TCM) -- small SRAM blocks that bypass the cache entirely and provide fixed-latency access, similar to the TCM on Cortex-M7. TCM on Cortex-A is less common and typically used for critical interrupt handlers or real-time firmware running on a companion core.

The L1 and L2 caches also interact with the MMU's Translation Lookaside Buffer (TLB). Every virtual-to-physical address translation requires a TLB lookup, and a TLB miss triggers a page table walk through the cache hierarchy or DRAM. On workloads with large memory footprints or scattered access patterns, TLB misses can become a significant performance bottleneck. Huge pages (2 MB or 1 GB mappings instead of the default 4 KB) reduce TLB pressure but require careful memory allocation.

The practical upshot for someone coming from MCU work: you can no longer look at a piece of code and predict how long it will take to execute. The same function, called twice with the same arguments, may take wildly different amounts of time depending on the cache state. This is not a bug -- it is the fundamental tradeoff that makes Cortex-A cores fast on average. The caches exploit temporal and spatial locality to keep frequently accessed data close to the core, and the strategy works well for general-purpose computing. It works poorly for tasks that need guaranteed worst-case timing, which is why the MCU/MPU split exists in the first place.

## Common SoC Families

A handful of SoC families dominate the embedded Linux and SBC world. Each has a distinct character in terms of performance, documentation, community support, and intended market. Choosing an SoC is not just about the CPU core -- it is about the entire package: what peripherals are integrated, how good the Linux BSP (Board Support Package) is, whether mainline kernel support exists, and whether you can actually get the documentation you need to bring up a custom board.

**Broadcom BCM2xxx (Raspberry Pi).** The BCM2835 (Pi 1, Zero), BCM2837 (Pi 3), BCM2711 (Pi 4), and BCM2712 (Pi 5) form a progression from single-core ARM1176 through quad-core A53, A72, and A76. Broadcom's SoCs were historically poorly documented -- the BCM2835 peripherals document was released only under community pressure, and GPU documentation remains limited. What makes the Pi ecosystem work is not the silicon documentation but the software: a well-maintained kernel fork, extensive community support, and the Raspberry Pi Foundation's commitment to upstream Linux support. For learning embedded Linux, the Pi's ecosystem is unmatched. For production embedded work, the dependency on a single-source SBC form factor and Broadcom's opaque silicon can be limiting.

**Allwinner (H3, H5, H616, and others).** Allwinner targets the budget end: cheap media players, Android TV boxes, and low-cost SBCs like the Orange Pi and NanoPi families. The H3 (quad A7) and H5 (quad A53) are old but ubiquitous. The H616 (quad A53 with Mali-G31) is the current budget favorite. Allwinner's documentation situation is mixed -- datasheets are available but sometimes incomplete, and some register-level documentation has come from community reverse-engineering efforts. Mainline Linux support has improved dramatically thanks to the linux-sunxi community, but some peripherals (especially video encode/decode and GPU) still rely on vendor-patched kernels. These SoCs are attractive for cost-sensitive designs where you need Linux capability at the lowest possible BOM cost.

**Rockchip (RK3328, RK3399, RK3588).** Rockchip occupies the mid-to-high performance tier. The RK3399 (2x A72 + 4x A55) has been a workhorse for NAS devices, edge compute, and Chromebooks. The RK3588 (4x A76 + 4x A55, Mali-G610 GPU, 6-TOPS NPU) is the current flagship, found in boards like the Orange Pi 5 and Radxa Rock 5B. Rockchip provides more documentation than Broadcom and has been relatively cooperative with mainline Linux upstreaming, though driver support for newer peripherals (NPU, some video codecs) often lags the mainline kernel. The RK3588 is compelling for projects that need serious compute -- multi-camera systems, AI inference at the edge, or NAS with hardware transcoding -- but its power consumption (10-15 W under load) demands active cooling and a proper power supply.

**TI Sitara AM335x (BeagleBone).** The AM3358/AM3359 is a single Cortex-A8 core at 1 GHz with two PRU (Programmable Real-time Unit) co-processors. The BeagleBone Black and its variants built around this SoC are popular in industrial and educational settings. The Sitara's distinguishing feature is the PRUs: 200 MHz 32-bit RISC cores with deterministic, single-cycle GPIO access. They bridge the gap between the non-deterministic Linux world on the A8 and the hard real-time needs of motor control, LED driving, or custom protocol bitbanging. TI's documentation is excellent -- thousands of pages of technical reference manual, publicly available, with register-level detail. The community is smaller than Raspberry Pi's but technically deep. The AM335x is aging (Cortex-A8 is a single-core, in-order design from 2005), but TI's newer AM62x and AM64x parts carry the philosophy forward with A53 cores and updated PRUs. If your project involves motor control, industrial I/O, or any task that needs both Linux and cycle-accurate GPIO timing, the Sitara family deserves a close look.

**NXP i.MX 6/8 (Industrial and Automotive).** NXP's i.MX family targets industrial, automotive, and medical applications where long-term availability and rigorous documentation matter. The i.MX 6 (single/dual/quad A9) has been in production for over a decade and remains available for long-lifecycle designs. The i.MX 8M family (A53 + optional M4/M7 companion core) is the current generation for new designs. NXP provides comprehensive reference manuals, evaluation boards, and a Yocto-based BSP (Board Support Package). The tradeoff is cost -- i.MX parts and their required companion PMICs are more expensive than Allwinner or Rockchip equivalents -- and NXP's software stack is heavy, favoring their own Yocto layers over community-driven approaches. If you need a 10-year supply guarantee and a silicon vendor who answers support tickets, i.MX is the answer. If you are building a hobby project or a short-run product, the price premium is hard to justify.

The pattern across these families is clear: you are choosing a combination of silicon capability, software maturity, documentation quality, and supply chain characteristics. The "best" SoC depends entirely on the project constraints. For learning and prototyping, start with whatever has the strongest community -- today that is Raspberry Pi or the Rockchip-based boards. For a production design, the documentation and vendor support story matters more than raw specs, because you will inevitably hit a hardware issue that only the vendor can explain.

## Gotchas

- **Cache misses destroy determinism** -- A single L2 cache miss can add 50-200 cycles to an operation that normally completes in 5 cycles. Code that runs fast on average may have catastrophic worst-case latency. If your application has timing constraints, you must characterize worst-case behavior, not average behavior
- **Multi-core does not mean multi-threaded code runs faster automatically** -- An application must be explicitly written to use multiple threads, and the workload must be parallelizable. A single-threaded Python script on a quad-core A72 uses exactly one core. Adding cores helps throughput for concurrent workloads (a web server handling multiple requests), not latency for serial computation
- **GPU driver support on embedded SoCs is often poor** -- Mali GPUs in Allwinner and Rockchip SoCs historically relied on closed-source, Android-derived drivers that work poorly or not at all under mainline Linux. The Panfrost (Midgard/Bifrost Mali) and Lima (Mali-400) open-source drivers have improved the situation, but hardware video acceleration and OpenCL/Vulkan support remain incomplete on many SoCs
- **Not all cores in a big.LITTLE cluster are interchangeable at the ISA level** -- While ARMv8-A requires big and LITTLE cores to be architecturally compatible (same ISA), they may differ in supported optional features, cache sizes, and TLB geometry. Software that assumes uniform cache line sizes or TLB behavior across all cores can encounter subtle performance anomalies
- **SoC documentation is often under NDA or incomplete** -- Broadcom, Allwinner, and MediaTek are notorious for limited public documentation. Even when datasheets exist, critical details about undocumented registers, errata, and power management sequences may only be available through vendor relationships or community reverse-engineering. Budget time for discovering what the documentation does not tell you
- **Thermal throttling on small SBCs limits sustained performance** -- A Cortex-A72 at 1.8 GHz in a fanless plastic case will throttle within minutes under sustained load, dropping to 1.0-1.2 GHz. The quoted clock speed is a peak, not a sustained guarantee. Benchmarks run in short bursts do not reflect real-world throughput. If sustained performance matters, you need heatsinks, airflow, or active cooling -- and your thermal budget should be part of the design from the start
