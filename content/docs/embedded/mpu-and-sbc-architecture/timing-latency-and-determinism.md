---
title: "Timing, Latency & Determinism"
weight: 70
---

# Timing, Latency & Determinism

On an MCU, interrupt response is deterministic — the datasheet gives a worst-case cycle count, and you can trust it. On an MPU running Linux, "how fast will it respond" becomes "how fast will it usually respond, and what is the worst case nobody can guarantee." This page covers why MPUs are not deterministic, how to measure and improve latency, and when to stop fighting Linux and offload real-time work to a co-processor.

## Why MPUs Are Not Deterministic

The fundamental problem is that an MPU running Linux has many layers of hardware and software between an external event and your code's response to it, and each layer introduces variable delay. On an MCU, an interrupt fires, the hardware saves context in a fixed number of cycles, and your ISR runs. On an MPU, the same interrupt passes through an interrupt controller, the kernel's top-half handler, possibly a threaded IRQ bottom-half, the scheduler, and finally your user-space process — and every one of those stages can stall.

Caches are the first source of jitter. A cache hit returns data in a few cycles; a cache miss on a Cortex-A7 can cost 50-100 cycles for an L2 miss and far more if it goes all the way to DRAM. Since cache state depends on what other code has been running, you cannot predict whether your critical code path will be hot or cold. An MCU with tightly-coupled memory has fixed-latency access — no guessing required. On an MPU, the same function can run 10x faster or slower depending on cache luck.

Virtual memory adds another layer of unpredictability. Every memory access goes through the TLB (Translation Lookaside Buffer), which caches recent virtual-to-physical mappings. A TLB hit is fast, but a TLB miss forces a page table walk — the hardware must traverse multi-level page tables in memory. Worse, if the page is not even mapped (a page fault), the kernel must handle the fault, which might involve allocating physical memory, zeroing a page, or reading from swap on disk. A single page fault in a time-critical path can add milliseconds of latency. See [MMU, Virtual Memory & Address Spaces]({{< relref "/docs/embedded/mpu-and-sbc-architecture/mmu-virtual-memory-and-address-spaces" >}}) for the underlying mechanism.

The Linux scheduler is perhaps the most visible source of non-determinism. Your process shares the CPU with potentially hundreds of other threads, and the default CFS (Completely Fair Scheduler) optimizes for throughput and fairness, not response time. At any moment, a higher-priority kernel thread, a different user-space process, or even a kernel softirq handler can preempt your code. On a stock kernel, some code paths hold spinlocks or disable preemption for significant periods — during those windows, nothing else can run on that CPU, including your latency-sensitive code.

Interrupts themselves add overhead. On an MCU, your ISR runs directly. On Linux, the kernel's interrupt framework handles the hardware interrupt first — it acknowledges the interrupt controller, runs any registered top-half handler (which should be minimal), then schedules a bottom-half (softirq, tasklet, or threaded IRQ) for the real work. Only after the kernel is done does your handler or your user-space process get a chance to respond. This kernel overhead is typically 5-20 microseconds even in the best case, and much worse when the kernel is busy.

Multi-core systems add yet another dimension. While having multiple CPU cores seems like it should help — dedicate one core to real-time work — the reality is more complex. Shared caches between cores mean that activity on one core can evict cache lines needed by the real-time core. Inter-processor interrupts (IPIs) for TLB shootdowns (when one core changes page table mappings, all cores must invalidate their TLB entries) create unpredictable stalls. CPU frequency scaling (DVFS) can change execution speed mid-task. Even memory bus contention between cores introduces variable latency. You can mitigate some of these with CPU isolation (`isolcpus` kernel parameter) and interrupt affinity (`irqbalance` or manual `/proc/irq/*/smp_affinity` settings), but each mitigation is partial.

The combined effect of all these layers is sobering. Measured GPIO toggle latency on a stock Raspberry Pi 4 shows an average case around 5-10 microseconds from user space, but occasional spikes hit 500 microseconds or more. Under heavy I/O load, spikes exceeding a millisecond are observable. These are not rare edge cases — run the system long enough under realistic load and they will happen. For anything that needs guaranteed response times, this is the core problem.

Plotting a latency histogram makes the problem visceral. The distribution is not Gaussian — it has a sharp peak at the typical latency and a long, fat tail stretching toward the worst case. That tail represents the times when multiple jitter sources align: a cache-cold wake-up coincides with a TLB miss and a kernel lock being held. These compound events are rare but not impossible, and their latency is not the sum of individual worst cases but something harder to predict. This is fundamentally different from the MCU world where worst-case timing is deterministic and specified in the datasheet. See [Determinism & Timing]({{< relref "/docs/embedded/real-time-concepts/determinism-and-timing" >}}) for how MCUs handle this differently.

## Measuring Latency on Linux

You cannot improve what you cannot measure, and measuring latency on Linux requires specific tools and methodology. The most important principle: the number that matters is the worst case, not the average. An average latency of 10 microseconds is meaningless if the worst case is 2 milliseconds — in a real-time system, the tail kills you.

`cyclictest` is the standard tool for measuring real-time latency on Linux. It comes from the `rt-tests` package and works by running a high-priority thread that sets a timer, sleeps, then measures the difference between when it was supposed to wake up and when it actually woke up. The delta is the scheduling latency — how long the system took to deliver the wake-up. Typical invocation: `cyclictest -m -p 90 -t 1 -n -l 1000000` runs one thread at priority 90, using nanosleep, for a million iterations with memory locked. A perhaps surprising result: on a stock Raspberry Pi 4 kernel with no load, worst-case latency was around 50-150 microseconds. Under load (running `stress-ng` to exercise CPU, memory, and I/O simultaneously), the worst case climbed to 500 microseconds to several milliseconds. Those numbers tell you exactly what you are working with.

Stress testing is essential because worst-case latency only reveals itself under adverse conditions. `stress-ng` can exercise CPU, memory, I/O, and various kernel subsystems. `hackbench` generates heavy scheduling load by creating many processes that pass messages through pipes. Running `cyclictest` alongside these stress tools for extended periods (hours, not minutes) gives a realistic picture of system behavior. Running stress tests overnight is advisable — short tests miss the rare worst cases that are exactly what you need to find.

An oscilloscope provides ground truth that no software measurement can match. The technique: write a tight loop in a real-time-priority thread that toggles a GPIO pin as fast as possible, or toggles it in response to an external trigger on another pin. Measure the jitter on the scope. This eliminates any measurement overhead from `clock_gettime()` or `gettimeofday()` — you are seeing the actual physical timing of pin transitions. In practice, oscilloscope measurements consistently show slightly worse jitter than `cyclictest` reports, likely because `cyclictest` cannot account for the overhead of the measurement itself.

An instructive approach: set up a test where an external signal generator triggers a GPIO interrupt, and the interrupt handler (or a waiting user-space thread) toggles a different GPIO pin in response. Put both the trigger and the response on the scope and measure the time delta. Run this for hours while applying stress loads. The histogram of response times you build up on a modern scope tells the complete latency story — average, distribution shape, and worst case — in a way that no software tool can fake.

For diagnosing where latency comes from, the kernel provides powerful tracing tools. `ftrace` (function tracer) can trace every function call in the kernel, enabling you to see exactly which code path caused a delay. The `irqsoff` and `preemptoff` tracers specifically measure the longest period that interrupts or preemption were disabled — these are often the smoking gun for latency spikes. `perf sched` records scheduling events and can show you which thread preempted which, and for how long. Learning these tools takes effort, but they transform latency debugging from guesswork into forensics.

A practical workflow: first, run `cyclictest` under stress to establish baseline worst-case numbers. If the numbers are unacceptable, enable the `preemptoff` tracer (`echo preemptoff > /sys/kernel/debug/tracing/current_tracer`) and reproduce the load. The tracer records the longest non-preemptible section, including a full stack trace. This usually points directly at the offending driver or kernel path. From there, you can decide whether to fix the driver, work around it, or change your architecture. Without tracing, you are just guessing — and in practice, guessing about latency sources is almost always wrong.

## PREEMPT_RT: The Real-Time Patch

The PREEMPT_RT patch set (also called the RT patch or the real-time patch) is the most significant software improvement available for Linux real-time performance. It transforms the Linux kernel from a throughput-optimized system into one that can provide bounded worst-case latency, and it has been the subject of over 20 years of development effort.

The key change is making nearly all kernel code preemptible. In a stock kernel, code running inside a spinlock cannot be preempted — and some spinlocks are held for long periods during I/O, memory management, or filesystem operations. PREEMPT_RT converts most spinlocks to `rt_mutex` (sleeping mutexes), which means a high-priority thread can preempt a lower-priority thread even if the lower-priority thread holds a lock. The lock holder gets priority-boosted (priority inheritance) to prevent priority inversion, but the critical point is that the high-priority thread is no longer blocked waiting for the lock to be released.

Interrupt handlers also change. Stock Linux runs hardware interrupt handlers (top halves) with interrupts disabled, which creates non-preemptible windows. PREEMPT_RT converts most interrupt handlers to threaded IRQs — the hardware interrupt does minimal work (acknowledge, wake the thread), and the actual handler runs as a kernel thread with a priority that can be managed by the scheduler. This means a high-priority real-time thread can preempt even an interrupt handler if its priority is higher. You can see these threaded IRQ handlers in `ps` output — they show up as kernel threads with names like `irq/86-mmc1` and you can adjust their RT priority using `chrt`.

Another important change is to the timer subsystem. PREEMPT_RT uses high-resolution timers throughout, which improves the precision of `nanosleep()`, `timerfd`, and other timer-based wake-up mechanisms. On a stock kernel, timer resolution is often limited to the tick rate (typically 1ms or 250us depending on `CONFIG_HZ`). With PREEMPT_RT and high-resolution timers, wake-up precision improves to a few microseconds, which matters for periodic real-time tasks.

The practical result is substantial. On a Raspberry Pi 4, worst-case latency measured with `cyclictest` under heavy load tells the story: stock kernel showed spikes to 1-2 milliseconds, while the PREEMPT_RT kernel kept the worst case under 80 microseconds. On an STM32MP1 (Cortex-A7), the improvement was similar — worst case dropped from hundreds of microseconds to around 50 microseconds. These numbers are typical for ARM boards running PREEMPT_RT and represent a 10-20x improvement in worst-case behavior.

What it costs: throughput drops slightly because there are more context switches and the lock conversions add overhead to every lock acquisition. Not all kernel drivers are RT-safe — some may hold raw spinlocks (unconverted) or have other latency-unfriendly patterns. Some kernel features (like certain network packet processing paths) can interact poorly with PREEMPT_RT. And it is an additional maintenance burden — you need to configure and build the RT kernel, which may lag behind mainline releases.

The good news is that PREEMPT_RT has been progressively merged into mainline Linux over the past several years, and as of the 6.x kernel series, most of the core patches are in mainline. The goal is full mainline integration, which will eventually eliminate the separate patch set entirely. For embedded Linux distributions, Yocto and Buildroot both support building PREEMPT_RT kernels as a configuration option. Some vendor BSPs (Board Support Packages) ship an RT kernel variant. If your SoC vendor provides one, use it as a starting point — they have already dealt with driver compatibility issues specific to that platform. See [RTOS Fundamentals]({{< relref "/docs/embedded/real-time-concepts/rtos-fundamentals" >}}) for how dedicated real-time operating systems compare.

## Kernel Bypass and Bare-Metal Co-Processors

When even PREEMPT_RT is not fast enough — or when you need truly deterministic, cycle-accurate timing — the answer is to stop fighting Linux and offload the real-time work entirely. This is not an admission of defeat; it is a sound architectural pattern used in production systems everywhere. The idea of "kernel bypass" covers a spectrum of approaches, from dedicated hardware peripherals (DMA engines, hardware timers, SPI controllers that run autonomously) through specialized co-processor subsystems (PRU, FPGA fabric) to full companion MCU cores on the same die.

Even simple kernel bypass can help significantly. Many SoC peripherals can operate autonomously once configured — a hardware timer can generate a PWM signal, an SPI controller can clock out a buffer via DMA, an ADC can sample on a timer trigger and fill a DMA buffer — all without CPU involvement. These hardware-driven data paths have zero software jitter because no code runs during the operation. The CPU just sets things up and collects results. This is the easiest form of "real-time on Linux" and it is underutilized — many engineers jump to co-processors when a DMA-driven peripheral would have solved the problem.

The TI PRU (Programmable Real-Time Unit) on BeagleBone boards is one of the cleanest examples. The AM335x SoC includes two PRU cores: 200 MHz RISC processors with single-cycle I/O access to dedicated GPIO pins. The PRU has no caches, no MMU, no pipeline hazards to worry about — each instruction takes exactly one cycle (5 nanoseconds). You can bitbang protocols, generate precise timing signals, and count edges with nanosecond accuracy. The PRU communicates with the Linux side via shared memory (12 KB shared between both PRU cores and the ARM host) and interrupt events. The PRU is notable because it recovers the deterministic, close-to-hardware feel of MCU programming while still having a full Linux system available for everything else. The downside is a limited toolchain and a somewhat steep learning curve — PRU assembly is straightforward, but the C compiler and the Linux remoteproc framework for loading PRU firmware take effort to learn.

Cortex-M companion cores on heterogeneous SoCs are becoming increasingly common. The STM32MP1 pairs a Cortex-A7 (running Linux) with a Cortex-M4 (running FreeRTOS, bare metal, or any RTOS). NXP's i.MX 8M family pairs Cortex-A53 cores with a Cortex-M4 or M7. The Cortex-M core has its own tightly-coupled memory, its own interrupt controller, and its own peripherals — it operates exactly like a standalone MCU, with all the determinism that implies. The key difference from using a separate MCU chip is that the companion core shares the SoC die, shares some peripherals, and has a built-in communication mechanism.

The communication pattern is consistent across these platforms: Linux handles networking, filesystem I/O, user interface, and logging; the co-processor handles hard real-time tasks like motor control, precise ADC sampling, or protocol bitbanging. They communicate via shared memory regions (carved out of DRAM or using on-chip SRAM), mailbox interrupts (hardware interrupt lines dedicated to inter-processor signaling), and often RPMsg (Remote Processor Messaging), a Linux framework that provides a virtio-based message-passing layer between the cores. RPMsg gives you named channels with send/receive semantics, which is far more ergonomic than raw shared memory, though it adds a few microseconds of overhead per message.

One subtlety of the co-processor approach that is easy to underestimate: debugging becomes harder. You now have two execution environments, potentially two debuggers, two sets of logs, and asynchronous communication between them. When something goes wrong, you need to correlate events across the Linux side and the co-processor side. Timestamped logging on both sides with a shared clock reference helps, but the debugging ergonomics are genuinely worse than either a pure MCU project or a pure Linux project. This is the real cost of the hybrid approach — not silicon, not board area, but engineering time during development and debugging.

The architectural pattern here is worth internalizing: do not try to make Linux do something it was not designed for. Linux is excellent at networking, filesystems, process management, and running complex software stacks. It is poor at cycle-accurate timing. Putting each task on the right processor is cleaner, more reliable, and often easier to debug than fighting with real-time patches and priority tuning on a general-purpose kernel.

## Comparing MCU and MPU Timing

A side-by-side comparison makes the magnitude of the differences concrete. These are representative numbers from tested boards and well-documented benchmarks — specific hardware and configuration will vary, but the orders of magnitude are consistent. The point of this comparison is not to say that MPUs are bad at timing — it is to calibrate expectations so you choose the right tool.

It is also worth noting that these comparisons assume user-space code on the MPU side. Moving critical operations into a kernel driver or using hardware peripherals with DMA can dramatically improve MPU timing — but at the cost of writing kernel-level code, which raises the engineering complexity and the consequences of bugs.

GPIO interrupt response time illustrates the gap most clearly. A Cortex-M4 running at 168 MHz (STM32F4) responds to an external interrupt in about 12 cycles — roughly 0.07 microseconds, completely deterministic. A PREEMPT_RT Linux system on a Cortex-A7 (STM32MP1) using a kernel GPIO interrupt handler typically responds in 20-80 microseconds, with a long tail that depends on system load. A stock Linux kernel on a Cortex-A72 (Raspberry Pi 4) shows 50-500 microseconds from user space, with occasional spikes beyond a millisecond. That is a range of roughly 1,000x to 10,000x between the MCU and the stock Linux MPU case.

ADC sampling is another revealing comparison. On an MCU, you configure a hardware timer to trigger ADC conversions at an exact rate, and DMA transfers the results to a buffer without any CPU involvement. The sample timing is determined entirely by hardware and has essentially zero jitter. On Linux, attempting to sample an ADC from user space at a precise rate is an exercise in frustration — `usleep()` and `nanosleep()` have scheduling jitter, and you cannot guarantee that your process will wake up on time. The solution on an MPU is to use a kernel driver that handles the timing in kernel space (ideally with DMA), or to offload the sampling to a co-processor. Industrial data acquisition systems on Linux almost always use one of these approaches rather than user-space timing loops.

PWM generation tells a similar story. MCU hardware timer peripherals produce PWM signals with jitter measured in clock cycles — sub-nanosecond on a fast MCU. Linux user-space PWM, even using the sysfs or libgpiod interfaces, shows visible jitter on an oscilloscope. Kernel-space PWM drivers using hardware timer peripherals on the SoC are much better, but still not as clean as a dedicated MCU timer because the driver itself runs in a shared kernel environment.

Serial protocol timing is worth mentioning too, because it often drives MCU-vs-MPU decisions in practice. Bit-banging a custom protocol (say, WS2812 LED data with 400ns/800ns timing windows) is trivial on an MCU with a timer and GPIO — each bit transition is cycle-accurate. On Linux, bit-banging anything with sub-microsecond timing from user space is essentially impossible. Even from kernel space, it is fragile because interrupts and preemption can stretch a critical timing window. The standard solution on an MPU is to use a hardware peripheral (SPI, DMA-driven GPIO) to generate the waveform, or offload it to a co-processor. This is a clear case where understanding the timing constraints upfront saves significant redesign effort later.

The practical takeaway is a rough decision tree. For anything requiring better than about 100 microseconds worst-case response on an MPU, seriously consider PREEMPT_RT. For anything requiring better than about 20 microseconds, use a co-processor or a dedicated MCU. For anything requiring sub-microsecond determinism, you need bare-metal MCU code or a specialized peripheral like the PRU. These thresholds are approximate, but they serve as reliable design guidelines.

## Gotchas

- **cyclictest requires proper setup to give meaningful results.** It must run with RT scheduling priority (`SCHED_FIFO` or `SCHED_RR`), memory locking (`-m` flag, which calls `mlockall()`), and ideally CPU affinity to isolate it from other tasks. Running it with default scheduling produces useless numbers that understate the problem
- **PREEMPT_RT is not a magic fix on its own.** Your user-space code must also cooperate: use `SCHED_FIFO` scheduling policy, call `mlockall(MCL_CURRENT | MCL_FUTURE)` to prevent page faults, pre-fault your stack and heap at startup, and avoid system calls that can block unpredictably (like `malloc()` during real-time operation). A PREEMPT_RT kernel with default-priority user-space code gives you very little
- **Page faults in RT code paths destroy latency.** A single demand page fault can cost hundreds of microseconds to milliseconds. Call `mlockall()` early, then touch (pre-fault) every page your program will use — including the stack. Allocate all buffers at startup, not during real-time operation
- **The RT throttling safety net catches people off guard.** `/proc/sys/kernel/sched_rt_runtime_us` defaults to 950000 (950 ms per second), meaning RT tasks can only use 95% of CPU time. If your RT task tries to use more, it gets throttled and your latency explodes. This safety mechanism prevents a runaway RT task from locking up the system, but you need to know it exists. Set it to -1 to disable (with caution) or ensure your RT task does not monopolize the CPU
- **One bad driver ruins system-wide RT latency.** A single driver that disables preemption or holds a raw spinlock for a long time will create latency spikes regardless of how carefully you have tuned everything else. The `ftrace` `preemptoff` tracer is the tool for finding these — it reports the longest non-preemptible section and which code path caused it
- **`nice` is not RT priority.** The `nice` value only affects the CFS scheduler's fairness weighting — it does not provide real-time guarantees or preemption priority. A nice -20 process still runs under CFS and can be preempted by kernel work. For actual real-time behavior, you need `SCHED_FIFO` or `SCHED_RR` via `chrt` or `sched_setscheduler()`
- **Software latency measurement has its own overhead.** `clock_gettime(CLOCK_MONOTONIC)` is good but not free — the call itself takes tens to hundreds of nanoseconds depending on whether the kernel uses a vDSO fast path or a full system call. For ground truth, toggle a GPIO and measure on an oscilloscope. The scope does not lie
