---
title: "Where MPUs Fit in Embedded Design"
weight: 100
---

# Where MPUs Fit in Embedded Design

This page is the synthesis — pulling together the architectural concepts from the rest of the section into practical design guidance. The question is not "MCU or MPU?" in the abstract, but "what does the system need to do, and which architecture makes that easier?" Sometimes the answer is one or the other. Sometimes it is both.

## The Design Decision Space

The MCU-vs-MPU decision can be understood as existing in a three-dimensional space. The axes are real-time requirements, software complexity, and production constraints. Each axis pulls the design toward a different architecture, and most real systems do not sit neatly at an extreme on any of them.

The first axis is real-time behavior. Hard real-time means a missed deadline is a system failure — motor commutation, safety interlocks, protocol timing at the bit level. Soft real-time means latency matters but occasional misses are tolerable — audio streaming, display refresh, network packet handling. Best-effort means the system does the work when it gets to it — data logging, user interfaces, batch processing. MCUs dominate hard real-time because their interrupt response is deterministic and measurable in clock cycles. MPUs running Linux are best-effort by default, and even with PREEMPT_RT or Xenomai, they offer soft real-time at best. See [Timing, Latency & Determinism]({{< relref "timing-latency-and-determinism" >}}) for the details of why Linux cannot guarantee hard deadlines.

The second axis is software complexity. At one end sits a bare-metal loop that reads a sensor and toggles an output — a few hundred lines of C, no OS, no dependencies. At the other end sits a system running a web server, an MQTT broker, TLS-encrypted connections, a database, camera processing, and over-the-air updates — essentially a Linux server in a small box. MCUs handle the simple end well. As software complexity grows, you start reimplementing operating system services: a TCP/IP stack, a filesystem, a task scheduler, a memory allocator. Each one works, but each one is a fraction of the capability of the Linux equivalent, and each one is code you must maintain. At some point, the honest choice is to use an OS that already provides these services.

The third axis is production constraints: unit cost, power consumption, BOM complexity, and physical size. An MCU can cost under a dollar, run for years on a coin cell, and need only a crystal and decoupling caps. An MPU adds external DRAM, storage, a PMIC, and supporting passives — easily $5-15 in additional BOM cost, hundreds of milliamps minimum current draw, and a more complex PCB layout. For a product shipping millions of units, that cost difference is decisive. For a product shipping hundreds, it is often irrelevant compared to development time.

MCUs dominate the corner where all three axes are favorable: low software complexity, hard real-time, low cost. MPUs dominate the opposite corner: high software complexity, best-effort timing, feature-rich applications where BOM cost is secondary. The interesting designs — and the hard decisions — live in the middle, where you need some real-time behavior and some software sophistication and the budget is tight. That middle ground is where hybrid architectures and careful partitioning become important. See [MCU vs MPU]({{< relref "mcu-vs-mpu" >}}) for the detailed architectural comparison that underlies these tradeoffs.

## MCU + MPU Hybrid Architectures

The hybrid approach — putting both an application processor and a real-time microcontroller on the same die — is increasingly common, and it represents where industrial embedded design is heading. Rather than forcing a choice between MCU and MPU, hybrid SoCs give you both, with hardware mechanisms for communication between the two domains.

The STM32MP1 from ST pairs a Cortex-A7 (running Linux) with a Cortex-M4 (running bare-metal or FreeRTOS firmware) on a single chip. Linux handles networking, the filesystem, logging, and any user interface. The M4 handles motor control, sensor acquisition, or safety-critical functions that need deterministic timing. The two cores share a region of SRAM and communicate through hardware mailbox interrupts and RPMsg (Remote Processor Messaging), an asymmetric multiprocessing protocol built on shared memory ring buffers. NXP's i.MX 8M family follows the same pattern: Cortex-A53 cores (up to four, running Linux) alongside a Cortex-M4 or M7 for real-time work. TI's AM64x goes further, including Cortex-A53 cores, Cortex-M4F cores, and Cortex-R5F cores — the R5F being a real-time profile designed for safety-critical automotive applications. Each core class has its own strengths: the A-cores for general compute, the M-cores for low-power real-time work, the R-cores for safety-certified deterministic execution.

The communication layer between cores is where the complexity lives. OpenAMP (Open Asymmetric Multi-Processing) provides a framework for lifecycle management (the A-core boots the M-core, loads its firmware, can restart it) and messaging (RPMsg channels carry structured data between cores). In practice, you define a protocol: the Linux side sends configuration commands, the M-core streams sensor data back. The shared memory region must be carefully managed — both sides need to agree on buffer locations, sizes, and access patterns. Cache coherency can be an issue: the A-core's caches may hold stale data if the M-core writes to shared memory. Some SoCs provide hardware cache coherency for the shared region; others require explicit cache maintenance operations.

The most challenging aspect of hybrid designs is the development workflow. You maintain two separate codebases: a Linux application (built with Yocto or Buildroot, debugged with GDB over SSH or JTAG) and an M-core firmware image (built with a cross-compiler like arm-none-eabi-gcc, debugged with an SWD probe). The two builds are independent — different toolchains, different linker scripts, different debug sessions. Loading the M-core firmware happens from Linux, typically by writing the binary to `/lib/firmware/` and triggering the remoteproc framework. When something goes wrong in the inter-processor communication, you may need to debug both sides simultaneously, which means two debugger sessions, two sets of breakpoints, and a lot of patience.

Despite the complexity, this architecture is compelling in practice. Common examples include industrial controllers where a PLC-style real-time loop runs on the M-core while a web-based configuration interface runs on the A-core, motor drive systems where field-oriented control runs on the M-core at a fixed 10 kHz loop rate while Linux handles logging, network telemetry, and firmware updates, and medical devices where safety-critical monitoring runs on the M-core (easier to certify a small, deterministic firmware image) while data recording and display run under Linux. The pattern is consistent: hard real-time on the M-core, everything else on Linux.

## Software Complexity as the Driver

Software complexity is the factor most likely to push a design from MCU to MPU, and the transition point is often more obvious in retrospect than during initial design.

Networking is the first threshold. A basic TCP/IP stack on an MCU is achievable — lwIP runs on Cortex-M parts with a few tens of kilobytes of RAM, and it handles simple socket communication adequately. But add TLS encryption (which requires a cryptographic library, certificate management, and enough RAM for the handshake buffers), then add MQTT for IoT messaging, then add an HTTP server for device configuration, then add mDNS for service discovery, and you are deep into territory where the MCU is spending more cycles managing the network stack than doing its actual job. Linux's networking stack handles all of this natively, with battle-tested implementations, regular security patches, and standard configuration tools. The effort of integrating and maintaining a full networking stack on an MCU is not impossible — it is just not honest engineering if a $5 SoC running Linux does it better.

Graphical user interfaces follow a similar pattern. A touchscreen UI on an MCU is possible — LVGL (Light and Versatile Graphics Library) runs on Cortex-M parts with a frame buffer and produces genuinely impressive results for the constraints. But the UI toolkit is limited, font rendering is basic, and adding features like web views, video playback, or complex animations requires increasing amounts of custom code. On an MPU, you have access to Qt, GTK, web-based interfaces served from a local web server, or Wayland compositors with GPU-accelerated rendering. The gap in capability is large, and it widens as the UI requirements grow.

Machine learning inference is another inflection point. TensorFlow Lite Micro runs on MCUs and can execute small models — keyword spotting, simple anomaly detection, basic gesture recognition. But model size is constrained by available RAM (often under 1 MB), inference time is slow for anything beyond trivial models, and there is no GPU or NPU to accelerate computation. An MPU with a GPU (like the Jetson Nano's 128 CUDA cores) or a dedicated NPU (like the Coral Edge TPU or the NPUs in recent Qualcomm and MediaTek SoCs) runs models that are orders of magnitude larger, at speeds that enable real-time video processing. If the application involves a camera doing object detection, an MCU is simply the wrong tool.

Camera integration is perhaps the most clear-cut case. USB cameras and MIPI CSI cameras are trivially supported under Linux through the V4L2 (Video4Linux2) framework — `ffmpeg`, `OpenCV`, `GStreamer`, and countless other tools work out of the box. On a bare-metal MCU, camera integration means writing a CSI or parallel interface driver, managing DMA transfers for frame data, implementing image processing in firmware, and handling frame buffer management — all without an OS to coordinate memory or scheduling. It is doable on high-end MCUs like the i.MX RT1170 (which has a camera interface and LCD controller), but the development effort is disproportionate to the result compared to what Linux provides.

The key threshold is this: when a design starts reimplementing operating system services on an MCU — a filesystem, a network stack, a process scheduler, a memory allocator — an MPU running Linux may be the more honest choice. Every one of those reimplemented services is a maintenance burden, a potential source of bugs, and a fraction of the capability of the Linux equivalent. The MCU is the right choice when the software is simple enough that you do not need those services. When you do need them, the question becomes whether the cost, power, and complexity of an MPU are justified — and increasingly, they are.

## Cost, Power, and BOM Implications

An MPU is never just a chip. This is the lesson that catches people who have only worked with MCUs, where the microcontroller is essentially the entire system. An MPU SoC requires a supporting ecosystem of components, and each one adds cost, board space, and potential failure modes.

External DRAM is the most significant addition. MPU SoCs include a DDR memory controller but no on-chip memory large enough to run an OS — they need external DRAM packages routed with controlled-impedance traces at high speed. LPDDR4 is the current sweet spot for embedded Linux systems, offering a good balance of bandwidth, power consumption, and cost. But DRAM is not just a cost item — it is a layout challenge. DDR routing requires matched-length traces, proper termination, and careful signal integrity work. This is why compute modules like the Raspberry Pi CM4 exist: they handle the SoC-to-DRAM routing on a small, validated module, letting the carrier board designer avoid the hardest part of the PCB layout.

Storage comes next. MCUs run code from internal flash — no external storage needed. An MPU running Linux needs a boot device and a root filesystem: eMMC (embedded MMC — a flash storage chip with a built-in controller, soldered to the board), NAND flash (raw flash requiring a controller in the SoC or a separate chip), or an SD card (fine for development, questionable for production as discussed in [Single-Board Computers as Systems]({{< relref "single-board-computers-as-systems" >}})). eMMC is the pragmatic choice for production — it provides wear leveling, error correction, and a standard interface, typically in the 4-64 GB range for a few dollars.

Power management is where MPU BOM complexity really shows. An MCU might need a single 3.3V rail, or at most a 3.3V and a 1.8V rail, provided by simple LDOs. An MPU SoC typically requires multiple voltage rails at precise levels: a core voltage (often 0.9-1.1V for the CPU cores), an I/O voltage (1.8V or 3.3V), a DRAM voltage (1.1V for LPDDR4), and possibly separate supplies for GPU, PLL, and USB blocks. These are provided by a PMIC — a power management IC that integrates multiple switching regulators and LDOs into one package, with sequencing logic to bring up the rails in the correct order. The PMIC alone can cost $2-5, and it needs its own set of inductors, capacitors, and feedback resistors. The bill of materials for the power section of an MPU board is often 30-50 components.

Adding it all up: an MPU-based design typically adds $5-15 or more to the BOM compared to an MCU that needs only a crystal, decoupling caps, and maybe a voltage regulator. For consumer electronics shipping in millions, this difference is enormous. For industrial equipment shipping in thousands, it is often less important than the development time saved by using Linux. For a one-off prototype, it is irrelevant.

Power consumption is the other dimension where MCUs and MPUs diverge sharply. A Cortex-M4 MCU in stop mode can draw single-digit microamps — the processor is essentially off, with only the RTC and maybe a few wake-up peripherals consuming power. A system with an MPU running Linux draws hundreds of milliamps as a floor: the DRAM must be refreshed constantly (DRAM is volatile — lose the refresh cycles and you lose the data), the SoC's PLLs and clocks are running, and the kernel itself has housekeeping tasks. There is no equivalent of "stop mode" that preserves a running Linux system at microamp-level current. In battery-powered designs, this dictates the architecture: an MCU handles the always-on sensing and wake-on-event logic, sleeping between measurements, while the MPU is powered off entirely or held in a deep suspend state. When something interesting happens — the sensor detects an event, enough data accumulates, or a timer expires — the MCU wakes the MPU, which boots, processes the data, communicates the results, and goes back to sleep. This MCU-as-gatekeeper pattern is common in remote monitoring, environmental sensing, and wearable devices.

## The Learning Path

MCU experience transfers well to MPU work, but the new skills required are substantial enough that the transition is not instant. Understanding what transfers and what is genuinely new helps set realistic expectations.

What transfers directly: the ability to read datasheets and reference manuals, understanding bus protocols and peripheral registers, debugging methodology (systematically isolating problems, using tools to observe system behavior), understanding of real-time constraints and timing analysis, familiarity with C and embedded build processes, and the general habit of thinking about hardware and software together. An engineer who has debugged a DMA transfer on an STM32 understands the concept of memory-mapped peripherals, bus arbitration, and interrupt-driven I/O — all of which exist on an MPU, just with more layers on top. See [MCU Architecture]({{< relref "/docs/embedded/mcu-architecture" >}}) for the fundamentals that carry forward.

What is genuinely new: the Linux kernel and its ecosystem. This is not one skill — it is a collection of them. Device tree (describing hardware to the kernel in a structured format rather than configuring peripherals with register writes), kernel drivers (writing or configuring modules that bridge hardware to user space), the user-space/kernel-space boundary (understanding why you cannot just `mmap` a register from an application, and when you can), build systems like Yocto and Buildroot (constructing a complete Linux image from source — kernel, bootloader, root filesystem, packages), and the boot chain from ROM to U-Boot to kernel to init system. Each of these is a topic unto itself, and the learning curve is real. See [Boot Chain]({{< relref "boot-chain" >}}) for the boot process specifically.

There is also a shift in debugging methodology. On an MCU, you connect a JTAG or SWD probe, set breakpoints, inspect registers, and step through code. That workflow exists on MPUs too (JTAG to the A-core, inspecting kernel state), but it is rarely the first tool you reach for. Linux debugging leans more heavily on log analysis (`dmesg`, `journalctl`, kernel ring buffer), tracing tools (`strace` for system call tracing, `ltrace` for library calls, `perf` for performance profiling, `ftrace` for kernel function tracing), and network-based debugging (SSH sessions, remote GDB, NFS-mounted root filesystems for rapid iteration). The shift from "attach a probe and set a breakpoint" to "read the logs and trace the calls" is a cultural change as much as a technical one.

Engineers with strong MCU backgrounds tend to adapt faster to MPU work than pure software engineers do. The MCU engineer already understands that the hardware is real and has constraints — clock frequencies, bus bandwidth, interrupt priorities, power states. The software engineer coming to embedded Linux often understands the OS concepts perfectly but struggles with the hardware realities: why a GPIO pin needs a pull-up resistor, why the UART baud rate clock has to be derived from a specific PLL, why the board does not boot when a particular voltage rail is marginal. The hardware intuition that MCU work builds is hard to acquire from software alone.

The reverse gap is also real. Software engineers bring skills that MCU engineers often lack: comfort with complex build systems, version control workflows for large codebases, package management, scripting, and systems administration. Both skill sets are valuable; the strongest embedded MPU engineers combine hardware intuition with Linux system fluency.

## Gotchas

- **Over-specifying the MPU** — choosing a quad-core Cortex-A72 when a single Cortex-A7 would suffice. The extra cores cost more, draw more power, generate more heat, and add complexity that the software never uses. Profile first, then select the SoC. A single A7 running at 800 MHz handles a surprising amount of embedded Linux workload
- **Under-specifying the MCU** — trying to run a complex networking stack with TLS, MQTT, and HTTP on a Cortex-M0+ with 32 KB of RAM. It might technically fit, but the result is fragile, difficult to maintain, and a fraction of the capability that Linux on a cheap A7 would provide. When the firmware starts feeling like a bad reimplementation of an operating system, it is time to reconsider the platform
- **Ignoring boot time** — an MCU is instant-on: power applied, firmware running within milliseconds. An MPU running Linux takes 5-30 seconds from power-on to a running application, depending on the boot chain, kernel configuration, and init system. This matters for automotive (the dashboard must respond immediately when the driver turns the key), safety systems (an emergency shutdown controller cannot wait 15 seconds to boot), and user experience (a consumer device that takes 20 seconds to turn on feels broken). Fast-boot optimizations exist (trimming the kernel, using a minimal init system, skipping unnecessary services) but they add development effort and still cannot match MCU instant-on
- **Neglecting long-term software maintenance** — an MCU firmware image can run unchanged for a decade with minimal maintenance. An MPU running Linux needs security patches (kernel CVEs, OpenSSL vulnerabilities, systemd bugs), kernel updates (upstream support for the SoC may be dropped, requiring backporting), and userland maintenance (package dependencies, library compatibility). The total cost of ownership for an MPU-based product over a 10-year lifecycle includes significant ongoing software engineering, not just the initial development
- **Prototype-to-production gap** — a Raspberry Pi prototype works on the bench, but it does not map cleanly to a production board. The boot process is Pi-specific (VideoCore GPU boot, `config.txt`), the device tree is Pi-specific, the power design is different, and the SoC (Broadcom BCM2711/2712) may not be available for custom designs in the way that ST, NXP, or TI parts are. Moving from Pi prototype to production means choosing a different SoC, writing a new device tree, building a new BSP, and redesigning the power system — essentially starting the hardware design over. Compute modules (Pi CM4/CM5) or evaluation boards from SoC vendors with production-friendly parts reduce this gap
- **Assuming "just add Linux" solves the problem** — Linux adds extraordinary capability, but it also adds complexity: boot time, kernel configuration, driver integration, filesystem management, security surface area, update mechanisms, and debugging difficulty. If the system can be done well on an MCU — if the firmware is straightforward, the I/O is deterministic, and the networking requirements are modest — that is often the better engineering choice. An MCU solution that works reliably is superior to an MPU solution that works most of the time but requires ongoing maintenance, generates heat, draws power, and introduces failure modes that did not exist before
